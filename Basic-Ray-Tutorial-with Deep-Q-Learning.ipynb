{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Ray Tutorial and Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial focuses on the cart-pole problem. A cart has a pole fixed with a movable lever in the middle of the cart. The cart slides along a frictionless surface. The goal is to keep the pole upright at all times. The test is how far back and forth the cart can move in order to prevent the pole from falling. The tutorial has been modified heavily so that it (i) runs in a jupyter notebook, (ii) demonstrates full capabilities of ray, and ray tune and (iii) breaks down the components of a RL project along with enhanced explainations of the code. We may modify this tutorial further to solve a different problem.\n",
    "\n",
    "In the second part of the tutorial, we demonstrate how to create a custom reinforcement learning environment with the problem space of a robot walking down a corridor.\n",
    "\n",
    "#### References:\n",
    "\n",
    "Barto, A. G., Sutton, R. S. and Anderson, C. (1983), ‘Neuron-like adaptive elements that can solve difficult learning control problems’, IEEE Transactions on Systems, 5, Man, and Cybernetics 13, 834–846\n",
    "\n",
    "Tune: A Research Platform for Distributed Model Selection and Training, Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion, arXiv preprint arXiv:1807.05118}, 2018\n",
    "\n",
    "Ray RLLib Documentation: [Ray RLLib Documentation](https://docs.ray.io/en/master/rllib.html)\n",
    "\n",
    "Ray Tune Documentation: [Ray Tune Documentation](https://docs.ray.io/en/master/tune/index.html)\n",
    "\n",
    "Mastering Reinforcement Learning with Python, Enes Bilgin, Packt Publishing, 2020 [Buy MRL with Python](https://www.amazon.com/Mastering-Reinforcement-Learning-Python-next-generation/dp/1838644148/?tag=meastus-200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ray --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if using Domino's ray, start ray this way\n",
    "\n",
    "import ray\n",
    "import os\n",
    "\n",
    "if ray.is_initialized() == False:\n",
    "   service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "   service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "   ray.util.connect(f\"{service_host}:{service_port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ray and what can it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import ray\n",
    "\n",
    "\n",
    "y = 1\n",
    "object_ref = y\n",
    "\n",
    "def add(x, a=1):\n",
    "    if x == 'add':\n",
    "        answer = a + 1\n",
    "    else:\n",
    "        answer = a\n",
    "    time.sleep(5)\n",
    "    print(answer)\n",
    "    \n",
    "number_add =add('add')\n",
    "number_none =add('hello')\n",
    "        \n",
    "object_ids = []\n",
    "st = time.time()\n",
    "for x in range(2):\n",
    "    x = x\n",
    "    y_id = add('add')\n",
    "    object_ids.append(y_id) # the object ids will print out\n",
    "    \n",
    "## getting the results to pass to another function\n",
    "objects = object_ids\n",
    "end = time.time()\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating remote objects\n",
    "\n",
    "Put an object in Ray's object store, get it out and run the function\n",
    "say want to add 10 million and after every million 5 seconds, total processing would be 50 seconds\n",
    "\n",
    "Do this in ray, and have 10 ray workers, adding 1 million values each, \n",
    "after calculating 1 million each sleeps 5 seconds, then total processing takes less than six seconds\n",
    "iterations in learning \n",
    "ml is already iterative, running partitions on each worker and at the distributed sequentially now paralellized\n",
    "call without ray and then with ray\n",
    "small amount of data, run and then kick off with same code but a larger data set, locally and in cloud testuse 10 workers, each sleeps 2 seconds, and see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "\n",
    "#ray.init()\n",
    "\n",
    "y = 1\n",
    "object_ref = ray.put(y)\n",
    "\n",
    "@ray.remote\n",
    "def add(x, a=1):\n",
    "    if x == 'add':\n",
    "        answer = a + 1\n",
    "    else:\n",
    "        answer = a\n",
    "    time.sleep(5)\n",
    "    print(answer)\n",
    "    \n",
    "number_add = ray.get(add.remote('add'))\n",
    "number_none = ray.get(add.remote('hello'))\n",
    "        \n",
    "object_ids = []\n",
    "st = time.time()\n",
    "for x in range(2):\n",
    "    x = x\n",
    "    y_id = add.remote('add')\n",
    "    object_ids.append(y_id) # the object ids will print out\n",
    "    \n",
    "## getting the results to pass to another function\n",
    "objects = ray.get(object_ids)\n",
    "end = time.time()\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_multiple():\n",
    "    time.sleep(5)\n",
    "    return 1, 2, 3\n",
    "\n",
    "st = time.time()\n",
    "a, b, c = return_multiple()\n",
    "print(a,b,c)\n",
    "end = time.time()\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_returns=3)\n",
    "def return_multiple():\n",
    "    time.sleep(5)\n",
    "    return 1, 2, 3\n",
    "\n",
    "\n",
    "a, b, c = return_multiple.remote()\n",
    "st = time.time()\n",
    "print(ray.get(a), ray.get(b), ray.get(c))\n",
    "end = time.time()\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculating pi\n",
    "import random\n",
    "\n",
    "NUM_SAMPLES = 15\n",
    "\n",
    "#@ray.remote\n",
    "def inside():\n",
    " x, y = random.random(), random.random()\n",
    " return x*x + y*y\n",
    "\n",
    "st = time.time()\n",
    "number = inside()\n",
    "end = time.time()\n",
    "print('The answer is: ', number)\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_SAMPLES = 15\n",
    "\n",
    "@ray.remote\n",
    "def inside():\n",
    " x, y = random.random(), random.random()\n",
    " return x*x + y*y\n",
    "\n",
    "st = time.time()\n",
    "number = ray.get(inside.remote())\n",
    "end = time.time()\n",
    "print('The answer is: ', number)\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cart Pole Problem\n",
    "\n",
    "Training with hyperparameter tuning was traditionally very human-time intensive. With the Ray 'tune' tool, hyper-parameter tuning is automated.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a RL Model using RLLib\n",
    "\n",
    "RLlib is an open-source library for reinforcement learning that offers both high scalability and a unified API for a variety of applications. RLlib natively supports TensorFlow, TensorFlow Eager, and PyTorch, but most of its internals are framework agnostic. See the docs [here](https://docs.ray.io/en/latest/rllib.html) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib import agents\n",
    "import pprint as pp\n",
    "import gym\n",
    "\n",
    "ray.init()\n",
    "\n",
    "config = {'gamma': 0.9,       \n",
    "          'lr': 1e-2,\n",
    "          'num_workers': 3,\n",
    "          'train_batch_size': 1000,\n",
    "          'model': {\n",
    "              'fcnet_hiddens': [128, 128]\n",
    "          }}\n",
    "\n",
    "trainer = agents.dqn.DQNTrainer(env='CartPole-v0') #test vanilla deep q network\n",
    "#trainer = agents.dqn.ApexTrainer(env='CartPole-v0') #test APEX optimized deep q network if using GPUs\n",
    "results = trainer.train()\n",
    "pp.pprint(results)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print path to logs\n",
    "!ls ~/ray_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# choose the path to your output logs\n",
    "logs_path = 'DQN_CartPole-v0_2021-11-16_10-29-197o_27f73'\n",
    "data_path = '~/ray_results/{}/progress.csv'.format(logs_path)\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running on your local machine use the below to access tensorboard\n",
    "#!tensorboard --logdir=logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create dataframe with pertinent information and graph the episode reward mean against the episode  per iteration\n",
    "\n",
    "#episode_reward_mean = df['episode_reward_mean']\n",
    "episodes_this_iter = df['episodes_this_iter']\n",
    "episodes_total = df['episodes_total']\n",
    "#episodes_reward_mean = df['evaluation/episode_reward_mean']\n",
    "#episodes_reward_max = df['evaluation/episode_reward_max']\n",
    "#episodes_reward_min = df['evaluation/episode_reward_min']\n",
    "\n",
    "df_episodes = pd.DataFrame(episodes_total)\n",
    "df_episodes['episodes_reward_mean'] = df['episode_reward_mean']\n",
    "df_episodes[\"episodes_reward_min\"] = df['episode_reward_min']\n",
    "df_episodes[\"episodes_reward_max\"] = df['episode_reward_max']\n",
    "\n",
    "df_episodes.plot.line()\n",
    "\n",
    "## here we see the total number of episodes has increased overall but in each iteration, fewer episodes are required \n",
    "## to achieve a higher reward.  We see the algorithm learning more quickly towards the end when it reaches its maximum\n",
    "## iterations and thus its best rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = trainer.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Environment and Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a custom environment; adapted from Ray documentation\n",
    "\n",
    "\"\"\"Example of a custom gym environment and model. Run this for a demo.\n",
    "\n",
    "This example shows:\n",
    "  - using a custom environment\n",
    "  - using a custom model\n",
    "  - using Tune for grid search\n",
    "\n",
    "You can visualize experiment results in ~/ray_results using TensorBoard.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import grid_search\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "run = 'DQN'\n",
    "framework = 'torch'\n",
    "as_test=\"store_true\"\n",
    "stop_iters = 50\n",
    "stop_timesteps = 100000\n",
    "stop_reward = 0.1\n",
    "\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Example of a custom env in which you have to walk down a corridor.\n",
    "\n",
    "    You can configure the length of the corridor via the env config.\"\"\"\n",
    "\n",
    "    def __init__(self, config: EnvContext):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = Discrete(2)\n",
    "        self.observation_space = Box(\n",
    "            0.0, self.end_pos, shape=(1, ), dtype=np.float32)\n",
    "        # Set the seed. This is only used for the final (reach goal) reward.\n",
    "        self.seed(config.worker_index * config.num_workers)\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos = 0\n",
    "        return [self.cur_pos]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1], action\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        done = self.cur_pos >= self.end_pos\n",
    "        # Produce a random reward when we reach the goal.\n",
    "        return [self.cur_pos], \\\n",
    "            random.random() * 2 if done else -0.1, done, {}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)\n",
    "\n",
    "\n",
    "class CustomModel(TFModelV2):\n",
    "    \"\"\"Example of a keras custom model that just delegates to an fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,\n",
    "                                          model_config, name)\n",
    "        self.model = FullyConnectedNetwork(obs_space, action_space,\n",
    "                                           num_outputs, model_config, name)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        return self.model.forward(input_dict, state, seq_lens)\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.model.value_function()\n",
    "\n",
    "\n",
    "class TorchCustomModel(TorchModelV2, nn.Module):\n",
    "    \"\"\"Example of a PyTorch custom model that just delegates to a fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.torch_sub_model = TorchFC(obs_space, action_space, num_outputs,\n",
    "                                       model_config, name)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        input_dict[\"obs\"] = input_dict[\"obs\"].float()\n",
    "        fc_out, _ = self.torch_sub_model(input_dict, state, seq_lens)\n",
    "        return fc_out, []\n",
    "\n",
    "    def value_function(self):\n",
    "        return torch.reshape(self.torch_sub_model.value_function(), [-1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is modified from the Ray documents.  The original can be found here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run and create the environment; change to work in a notebook\n",
    "\n",
    "import ray\n",
    "from ray.tune import grid_search\n",
    "\n",
    "import os\n",
    "local_dir = '/domino/datasets/local/{}'.format(os.environ['DOMINO_PROJECT_NAME'])\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Can also register the env creator function explicitly with:\n",
    "# register_env(\"corridor\", lambda config: SimpleCorridor(config))\n",
    "\n",
    "ModelCatalog.register_custom_model(\n",
    "    \"my_model\", TorchCustomModel\n",
    "    if framework == \"torch\" else CustomModel)\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleCorridor,  # or \"corridor\" if registered above\n",
    "    \"env_config\": {\n",
    "        \"corridor_length\": 5,\n",
    "    },\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"my_model\",\n",
    "        \"vf_share_layers\": True,\n",
    "    },\n",
    "    \"lr\": grid_search([1e-2, 1e-4, 1e-6]),  # try different lrs\n",
    "    \"num_workers\": 3,  # parallelism\n",
    "    \"framework\": framework,\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": stop_iters,\n",
    "    \"timesteps_total\": stop_timesteps,\n",
    "    \"episode_reward_mean\": stop_reward,\n",
    "}\n",
    "\n",
    "results = tune.run(run, config=config, stop=stop, local_dir = local_dir)\n",
    "\n",
    "if as_test:\n",
    "    check_learning_achieved(results, stop_reward)\n",
    "    \n",
    "print(\"best config: \", results.get_best_config(metric = 'episode_reward_mean', mode = 'max'))\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finance Example: Predicting Investing Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "\n",
    "We have two notebooks remaining:\n",
    "\n",
    "*Using Pytorch and Ray for a simple finance example using DQN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
