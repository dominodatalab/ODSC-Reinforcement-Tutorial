{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Ray Tutorial and Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial focuses on the cart-pole problem. A cart has a pole fixed with a movable lever in the middle of the cart. The cart slides along a frictionless surface. The goal is to keep the pole upright at all times. The test is how far back and forth the cart can move in order to prevent the pole from falling. The tutorial has been modified heavily so that it (i) runs in a jupyter notebook, (ii) demonstrates full capabilities of ray, and ray tune and (iii) breaks down the components of a RL project along with enhanced explainations of the code. We may modify this tutorial further to solve a different problem.\n",
    "\n",
    "In the second part of the tutorial, we demonstrate how to create a custom reinforcement learning environment with the problem space of a robot walking down a corridor.\n",
    "\n",
    "#### References:\n",
    "\n",
    "Barto, A. G., Sutton, R. S. and Anderson, C. (1983), ‘Neuron-like adaptive elements that can solve difficult learning control problems’, IEEE Transactions on Systems, 5, Man, and Cybernetics 13, 834–846\n",
    "\n",
    "Tune: A Research Platform for Distributed Model Selection and Training, Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion, arXiv preprint arXiv:1807.05118}, 2018\n",
    "\n",
    "Ray RLLib Documentation: [Ray RLLib Documentation](https://docs.ray.io/en/master/rllib.html)\n",
    "\n",
    "Ray Tune Documentation: [Ray Tune Documentation](https://docs.ray.io/en/master/tune/index.html)\n",
    "\n",
    "Mastering Reinforcement Learning with Python, Enes Bilgin, Packt Publishing, 2020 [Buy MRL with Python](https://www.amazon.com/Mastering-Reinforcement-Learning-Python-next-generation/dp/1838644148/?tag=meastus-200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user ray==1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ray --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if using Domino's ray, start ray this way\n",
    "\n",
    "import ray\n",
    "import os\n",
    "\n",
    "if ray.is_initialized() == False:\n",
    "   service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "   service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "   ray.util.connect(f\"{service_host}:{service_port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ray and what can it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import ray\n",
    "\n",
    "\n",
    "y = 1\n",
    "object_ref = y\n",
    "\n",
    "def add(x, a=1):\n",
    "    if x == 'add':\n",
    "        answer = a + 1\n",
    "    else:\n",
    "        answer = a\n",
    "    time.sleep(5)\n",
    "    print(answer)\n",
    "    \n",
    "number_add =add('add')\n",
    "number_none =add('hello')\n",
    "        \n",
    "object_ids = []\n",
    "st = time.time()\n",
    "for x in range(2):\n",
    "    x = x\n",
    "    y_id = add('add')\n",
    "    object_ids.append(y_id) # the object ids will print out\n",
    "    \n",
    "## getting the results to pass to another function\n",
    "objects = object_ids\n",
    "end = time.time()\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "\n",
    "## creating remote objects\n",
    "\n",
    "# Put an object in Ray's object store, get it out and run the function\n",
    "# say want to add 10 million and after every million 5 seconds, total processing would be 50 seconds\n",
    "# do this in ray, and have 10 ray workers, adding 1 million values each, after calculating 1 million each sleeps\n",
    "# 5 seconds, then total processing takes less than six seconds\n",
    "# iterations in learning \n",
    "# ml is already iterative, running partitions on each worker and at the distributed sequentially now paralellized\n",
    "# call without ray and then with ray\n",
    "# small amount of data, run and then kick off with same code but a larger data set, locally and in cloud test\n",
    "# use 10 workers, each sleeps 2 seconds, and see the difference\n",
    "\n",
    "#ray.init()\n",
    "\n",
    "y = 1\n",
    "object_ref = ray.put(y)\n",
    "\n",
    "@ray.remote\n",
    "def add(x, a=1):\n",
    "    if x == 'add':\n",
    "        answer = a + 1\n",
    "    else:\n",
    "        answer = a\n",
    "    time.sleep(5)\n",
    "    print(answer)\n",
    "    \n",
    "number_add = ray.get(add.remote('add'))\n",
    "number_none = ray.get(add.remote('hello'))\n",
    "        \n",
    "object_ids = []\n",
    "st = time.time()\n",
    "for x in range(2):\n",
    "    x = x\n",
    "    y_id = add.remote('add')\n",
    "    object_ids.append(y_id) # the object ids will print out\n",
    "    \n",
    "## getting the results to pass to another function\n",
    "objects = ray.get(object_ids)\n",
    "end = time.time()\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_multiple():\n",
    "    time.sleep(5)\n",
    "    return 1, 2, 3\n",
    "\n",
    "st = time.time()\n",
    "a, b, c = return_multiple()\n",
    "print(a,b,c)\n",
    "end = time.time()\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_returns=3)\n",
    "def return_multiple():\n",
    "    time.sleep(5)\n",
    "    return 1, 2, 3\n",
    "\n",
    "\n",
    "a, b, c = return_multiple.remote()\n",
    "st = time.time()\n",
    "print(ray.get(a), ray.get(b), ray.get(c))\n",
    "end = time.time()\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculating pi\n",
    "import random\n",
    "\n",
    "NUM_SAMPLES = 15\n",
    "\n",
    "#@ray.remote\n",
    "def inside():\n",
    " x, y = random.random(), random.random()\n",
    " return x*x + y*y\n",
    "\n",
    "st = time.time()\n",
    "number = inside()\n",
    "end = time.time()\n",
    "print('The answer is: ', number)\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_SAMPLES = 15\n",
    "\n",
    "@ray.remote\n",
    "def inside():\n",
    " x, y = random.random(), random.random()\n",
    " return x*x + y*y\n",
    "\n",
    "st = time.time()\n",
    "number = ray.get(inside.remote())\n",
    "end = time.time()\n",
    "print('The answer is: ', number)\n",
    "print(str(end-st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cart Pole Problem\n",
    "\n",
    "Training with hyperparameter tuning was traditionally very human-time intensive. With the Ray 'tune' tool, hyper-parameter tuning is automated.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a RL Model using RLLib\n",
    "\n",
    "loreum ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ay (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ay (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: ray==1.7 in /home/ubuntu/.local/lib/python3.8/site-packages (1.7.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.8/site-packages (from ray==1.7) (21.2.0)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /opt/conda/lib/python3.8/site-packages (from ray==1.7) (3.16.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.8/site-packages (from ray==1.7) (1.19.5)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.8/site-packages (from ray==1.7) (8.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from ray==1.7) (3.0.12)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /opt/conda/lib/python3.8/site-packages (from ray==1.7) (1.39.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ray==1.7) (1.0.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from ray==1.7) (5.4.1)\n",
      "Requirement already satisfied: redis>=3.5.0 in /opt/conda/lib/python3.8/site-packages (from ray==1.7) (3.5.3)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.8/site-packages (from grpcio>=1.28.1->ray==1.7) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ay (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ay (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ay (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ay (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user ray==1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 15:24:18,067\tWARNING deprecation.py:38 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "2021-10-20 15:24:18,069\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 1000,\n",
      " 'config': {'_fake_gpus': False,\n",
      "            '_tf_policy_handles_more_than_one_loss': False,\n",
      "            'action_space': None,\n",
      "            'actions_in_input_normalized': False,\n",
      "            'adam_epsilon': 1e-08,\n",
      "            'batch_mode': 'truncate_episodes',\n",
      "            'before_learn_on_batch': None,\n",
      "            'buffer_size': 50000,\n",
      "            'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      "            'clip_actions': False,\n",
      "            'clip_rewards': None,\n",
      "            'collect_metrics_timeout': 180,\n",
      "            'compress_observations': False,\n",
      "            'create_env_on_driver': False,\n",
      "            'custom_eval_function': None,\n",
      "            'custom_resources_per_worker': {},\n",
      "            'double_q': True,\n",
      "            'dueling': True,\n",
      "            'eager_tracing': False,\n",
      "            'env': 'CartPole-v0',\n",
      "            'env_config': {},\n",
      "            'env_task_fn': None,\n",
      "            'evaluation_config': {'explore': False},\n",
      "            'evaluation_interval': None,\n",
      "            'evaluation_num_episodes': 10,\n",
      "            'evaluation_num_workers': 0,\n",
      "            'evaluation_parallel_to_training': False,\n",
      "            'exploration_config': {'epsilon_timesteps': 10000,\n",
      "                                   'final_epsilon': 0.02,\n",
      "                                   'initial_epsilon': 1.0,\n",
      "                                   'type': 'EpsilonGreedy'},\n",
      "            'explore': True,\n",
      "            'extra_python_environs_for_driver': {},\n",
      "            'extra_python_environs_for_worker': {},\n",
      "            'fake_sampler': False,\n",
      "            'final_prioritized_replay_beta': 0.4,\n",
      "            'framework': 'tf',\n",
      "            'gamma': 0.99,\n",
      "            'grad_clip': 40,\n",
      "            'hiddens': [256],\n",
      "            'horizon': None,\n",
      "            'ignore_worker_failures': False,\n",
      "            'in_evaluation': False,\n",
      "            'input': 'sampler',\n",
      "            'input_config': {},\n",
      "            'input_evaluation': ['is', 'wis'],\n",
      "            'learning_starts': 1000,\n",
      "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                                      'intra_op_parallelism_threads': 8},\n",
      "            'log_level': 'WARN',\n",
      "            'log_sys_usage': True,\n",
      "            'logger_config': None,\n",
      "            'lr': 0.0005,\n",
      "            'lr_schedule': None,\n",
      "            'metrics_smoothing_episodes': 100,\n",
      "            'min_iter_time_s': 1,\n",
      "            'model': {'_no_preprocessing': False,\n",
      "                      '_time_major': False,\n",
      "                      '_use_default_native_models': False,\n",
      "                      'attention_dim': 64,\n",
      "                      'attention_head_dim': 32,\n",
      "                      'attention_init_gru_gate_bias': 2.0,\n",
      "                      'attention_memory_inference': 50,\n",
      "                      'attention_memory_training': 50,\n",
      "                      'attention_num_heads': 1,\n",
      "                      'attention_num_transformer_units': 1,\n",
      "                      'attention_position_wise_mlp_dim': 32,\n",
      "                      'attention_use_n_prev_actions': 0,\n",
      "                      'attention_use_n_prev_rewards': 0,\n",
      "                      'conv_activation': 'relu',\n",
      "                      'conv_filters': None,\n",
      "                      'custom_action_dist': None,\n",
      "                      'custom_model': None,\n",
      "                      'custom_model_config': {},\n",
      "                      'custom_preprocessor': None,\n",
      "                      'dim': 84,\n",
      "                      'fcnet_activation': 'tanh',\n",
      "                      'fcnet_hiddens': [256, 256],\n",
      "                      'framestack': True,\n",
      "                      'free_log_std': False,\n",
      "                      'grayscale': False,\n",
      "                      'lstm_cell_size': 256,\n",
      "                      'lstm_use_prev_action': False,\n",
      "                      'lstm_use_prev_action_reward': -1,\n",
      "                      'lstm_use_prev_reward': False,\n",
      "                      'max_seq_len': 20,\n",
      "                      'no_final_linear': False,\n",
      "                      'post_fcnet_activation': 'relu',\n",
      "                      'post_fcnet_hiddens': [],\n",
      "                      'use_attention': False,\n",
      "                      'use_lstm': False,\n",
      "                      'vf_share_layers': True,\n",
      "                      'zero_mean': True},\n",
      "            'monitor': -1,\n",
      "            'multiagent': {'count_steps_by': 'env_steps',\n",
      "                           'observation_fn': None,\n",
      "                           'policies': {'default_policy': PolicySpec(policy_class=None, observation_space=None, action_space=None, config={})},\n",
      "                           'policies_to_train': None,\n",
      "                           'policy_map_cache': None,\n",
      "                           'policy_map_capacity': 100,\n",
      "                           'policy_mapping_fn': None,\n",
      "                           'replay_mode': 'independent'},\n",
      "            'n_step': 1,\n",
      "            'no_done_at_end': False,\n",
      "            'noisy': False,\n",
      "            'normalize_actions': True,\n",
      "            'num_atoms': 1,\n",
      "            'num_cpus_for_driver': 1,\n",
      "            'num_cpus_per_worker': 1,\n",
      "            'num_envs_per_worker': 1,\n",
      "            'num_gpus': 0,\n",
      "            'num_gpus_per_worker': 0,\n",
      "            'num_workers': 0,\n",
      "            'observation_filter': 'NoFilter',\n",
      "            'observation_space': None,\n",
      "            'optimizer': {},\n",
      "            'output': None,\n",
      "            'output_compress_columns': ['obs', 'new_obs'],\n",
      "            'output_max_file_size': 67108864,\n",
      "            'placement_strategy': 'PACK',\n",
      "            'postprocess_inputs': False,\n",
      "            'preprocessor_pref': 'deepmind',\n",
      "            'prioritized_replay': True,\n",
      "            'prioritized_replay_alpha': 0.6,\n",
      "            'prioritized_replay_beta': 0.4,\n",
      "            'prioritized_replay_beta_annealing_timesteps': 20000,\n",
      "            'prioritized_replay_eps': 1e-06,\n",
      "            'record_env': False,\n",
      "            'remote_env_batch_wait_ms': 0,\n",
      "            'remote_worker_envs': False,\n",
      "            'render_env': False,\n",
      "            'replay_sequence_length': 1,\n",
      "            'rollout_fragment_length': 4,\n",
      "            'sample_async': False,\n",
      "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      "            'seed': None,\n",
      "            'shuffle_buffer_size': 0,\n",
      "            'sigma0': 0.5,\n",
      "            'simple_optimizer': False,\n",
      "            'soft_horizon': False,\n",
      "            'store_buffer_in_checkpoints': False,\n",
      "            'synchronize_filters': True,\n",
      "            'target_network_update_freq': 500,\n",
      "            'tf_session_args': {'allow_soft_placement': True,\n",
      "                                'device_count': {'CPU': 1},\n",
      "                                'gpu_options': {'allow_growth': True},\n",
      "                                'inter_op_parallelism_threads': 2,\n",
      "                                'intra_op_parallelism_threads': 2,\n",
      "                                'log_device_placement': False},\n",
      "            'timesteps_per_iteration': 1000,\n",
      "            'train_batch_size': 32,\n",
      "            'training_intensity': None,\n",
      "            'v_max': 10.0,\n",
      "            'v_min': -10.0,\n",
      "            'worker_side_prioritization': False},\n",
      " 'custom_metrics': {},\n",
      " 'date': '2021-10-20_15-24-19',\n",
      " 'done': False,\n",
      " 'episode_len_mean': 20.833333333333332,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': 59.0,\n",
      " 'episode_reward_mean': 20.833333333333332,\n",
      " 'episode_reward_min': 9.0,\n",
      " 'episodes_this_iter': 48,\n",
      " 'episodes_total': 48,\n",
      " 'experiment_id': '5cb1a776126c4bebbee3e4fd83a37b5b',\n",
      " 'hist_stats': {'episode_lengths': [14,\n",
      "                                    21,\n",
      "                                    19,\n",
      "                                    19,\n",
      "                                    59,\n",
      "                                    19,\n",
      "                                    9,\n",
      "                                    24,\n",
      "                                    10,\n",
      "                                    14,\n",
      "                                    49,\n",
      "                                    15,\n",
      "                                    20,\n",
      "                                    12,\n",
      "                                    21,\n",
      "                                    39,\n",
      "                                    20,\n",
      "                                    24,\n",
      "                                    32,\n",
      "                                    13,\n",
      "                                    17,\n",
      "                                    19,\n",
      "                                    12,\n",
      "                                    17,\n",
      "                                    16,\n",
      "                                    21,\n",
      "                                    12,\n",
      "                                    16,\n",
      "                                    19,\n",
      "                                    30,\n",
      "                                    12,\n",
      "                                    21,\n",
      "                                    32,\n",
      "                                    10,\n",
      "                                    11,\n",
      "                                    52,\n",
      "                                    12,\n",
      "                                    12,\n",
      "                                    22,\n",
      "                                    17,\n",
      "                                    15,\n",
      "                                    11,\n",
      "                                    14,\n",
      "                                    18,\n",
      "                                    13,\n",
      "                                    36,\n",
      "                                    14,\n",
      "                                    46],\n",
      "                'episode_reward': [14.0,\n",
      "                                   21.0,\n",
      "                                   19.0,\n",
      "                                   19.0,\n",
      "                                   59.0,\n",
      "                                   19.0,\n",
      "                                   9.0,\n",
      "                                   24.0,\n",
      "                                   10.0,\n",
      "                                   14.0,\n",
      "                                   49.0,\n",
      "                                   15.0,\n",
      "                                   20.0,\n",
      "                                   12.0,\n",
      "                                   21.0,\n",
      "                                   39.0,\n",
      "                                   20.0,\n",
      "                                   24.0,\n",
      "                                   32.0,\n",
      "                                   13.0,\n",
      "                                   17.0,\n",
      "                                   19.0,\n",
      "                                   12.0,\n",
      "                                   17.0,\n",
      "                                   16.0,\n",
      "                                   21.0,\n",
      "                                   12.0,\n",
      "                                   16.0,\n",
      "                                   19.0,\n",
      "                                   30.0,\n",
      "                                   12.0,\n",
      "                                   21.0,\n",
      "                                   32.0,\n",
      "                                   10.0,\n",
      "                                   11.0,\n",
      "                                   52.0,\n",
      "                                   12.0,\n",
      "                                   12.0,\n",
      "                                   22.0,\n",
      "                                   17.0,\n",
      "                                   15.0,\n",
      "                                   11.0,\n",
      "                                   14.0,\n",
      "                                   18.0,\n",
      "                                   13.0,\n",
      "                                   36.0,\n",
      "                                   14.0,\n",
      "                                   46.0]},\n",
      " 'hostname': 'run-6170309a3ddbad0df8b110a0-tm44l',\n",
      " 'info': {'last_target_update_ts': 1000,\n",
      "          'learner': {'default_policy': {'custom_metrics': {},\n",
      "                                         'learner_stats': {'cur_lr': 0.0005000000237487257,\n",
      "                                                           'max_q': 0.14237544,\n",
      "                                                           'mean_q': -0.06823551,\n",
      "                                                           'mean_td_error': -1.2996446,\n",
      "                                                           'min_q': -0.46522743,\n",
      "                                                           'model': {}},\n",
      "                                         'td_error': array([-2.3229191 , -2.3730245 , -0.57407844, -2.4302914 , -1.5434479 ,\n",
      "       -0.94303256, -2.5311205 , -2.4673476 , -0.45413026, -2.0513368 ,\n",
      "       -0.96815616, -0.84985185, -1.1343527 , -1.0352614 , -0.9681022 ,\n",
      "       -0.72281957, -0.60948205, -2.416604  , -2.580868  , -1.4218411 ,\n",
      "       -0.8600527 , -0.64454544, -0.53057444, -0.93047154, -1.3239115 ,\n",
      "       -0.42323545, -1.1177925 , -0.3741497 , -1.2647383 , -0.94589317,\n",
      "       -1.9617598 , -0.813434  ], dtype=float32)}},\n",
      "          'num_agent_steps_sampled': 1000,\n",
      "          'num_agent_steps_trained': 32,\n",
      "          'num_steps_sampled': 1000,\n",
      "          'num_steps_trained': 32,\n",
      "          'num_target_updates': 1},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '10.0.33.251',\n",
      " 'num_healthy_workers': 0,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 20.633333333333333, 'ram_util_percent': 18.6},\n",
      " 'pid': 1409,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {'mean_action_processing_ms': 0.04980423590996406,\n",
      "                  'mean_env_render_ms': 0.0,\n",
      "                  'mean_env_wait_ms': 0.05644351452380627,\n",
      "                  'mean_inference_ms': 0.9411662727683695,\n",
      "                  'mean_raw_obs_processing_ms': 0.11450617939799458},\n",
      " 'time_since_restore': 1.5539710521697998,\n",
      " 'time_this_iter_s': 1.5539710521697998,\n",
      " 'time_total_s': 1.5539710521697998,\n",
      " 'timers': {'learn_throughput': 191.76,\n",
      "            'learn_time_ms': 166.875,\n",
      "            'load_throughput': 92500.157,\n",
      "            'load_time_ms': 0.346},\n",
      " 'timestamp': 1634743459,\n",
      " 'timesteps_since_restore': 0,\n",
      " 'timesteps_total': 1000,\n",
      " 'training_iteration': 1}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib import agents\n",
    "import pprint as pp\n",
    "import gym\n",
    "\n",
    "#ray.init()\n",
    "\n",
    "config = {'gamma': 0.9,       \n",
    "          'lr': 1e-2,\n",
    "          'num_workers': 2,\n",
    "          'train_batch_size': 1000,\n",
    "          'model': {\n",
    "              'fcnet_hiddens': [128, 128]\n",
    "          }}\n",
    "trainer = agents.dqn.DQNTrainer(env='CartPole-v0') #test vanilla deep q network\n",
    "#trainer = agents.dqn.ApexTrainer(env='CartPole-v0') #test APEX optimized deep q network if using GPUs\n",
    "results = trainer.train()\n",
    "pp.pprint(results)\n",
    "\n",
    "#ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running on your local machine use the below to access tensorboard\n",
    "\n",
    "#!tensorboard --logdir='/mnt/data/Optiver_Realized_Volatility_Prediction_Challenge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>agent_timesteps_total</th>\n",
       "      <th>done</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>...</th>\n",
       "      <th>evaluation/sampler_perf/mean_inference_ms</th>\n",
       "      <th>evaluation/sampler_perf/mean_action_processing_ms</th>\n",
       "      <th>evaluation/sampler_perf/mean_env_wait_ms</th>\n",
       "      <th>evaluation/sampler_perf/mean_env_render_ms</th>\n",
       "      <th>info/learner/default_policy/td_error</th>\n",
       "      <th>info/learner/default_policy/learner_stats/cur_lr</th>\n",
       "      <th>info/learner/default_policy/learner_stats/mean_q</th>\n",
       "      <th>info/learner/default_policy/learner_stats/min_q</th>\n",
       "      <th>info/learner/default_policy/learner_stats/max_q</th>\n",
       "      <th>info/learner/default_policy/learner_stats/mean_td_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.255814</td>\n",
       "      <td>23.255814</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>1.175044</td>\n",
       "      <td>0.051374</td>\n",
       "      <td>0.055553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ 0.1490928   0.80672693  0.07395267 -0.229648...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>1.960149</td>\n",
       "      <td>1.149093</td>\n",
       "      <td>2.435233</td>\n",
       "      <td>0.055881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24.390244</td>\n",
       "      <td>24.390244</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.826491</td>\n",
       "      <td>0.047578</td>\n",
       "      <td>0.052969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.8847606  -0.14398265 -0.70628667 -0.102808...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>3.598083</td>\n",
       "      <td>1.941867</td>\n",
       "      <td>4.605349</td>\n",
       "      <td>-0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.220000</td>\n",
       "      <td>26.220000</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>False</td>\n",
       "      <td>116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819460</td>\n",
       "      <td>0.047618</td>\n",
       "      <td>0.053124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ 3.183545    0.0329237  -0.34508467 -0.671693...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>5.491560</td>\n",
       "      <td>0.578560</td>\n",
       "      <td>7.023629</td>\n",
       "      <td>0.171092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29.080000</td>\n",
       "      <td>29.080000</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>False</td>\n",
       "      <td>147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788918</td>\n",
       "      <td>0.046526</td>\n",
       "      <td>0.051812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-5.6033611e-01 -1.8792248e-01  2.8856370e+00 ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>7.154577</td>\n",
       "      <td>1.618476</td>\n",
       "      <td>9.023160</td>\n",
       "      <td>0.620341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>129.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35.350000</td>\n",
       "      <td>35.350000</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775734</td>\n",
       "      <td>0.046113</td>\n",
       "      <td>0.051508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-7.2457480e-01  4.5471525e-01  1.5333462e-01 ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>9.651964</td>\n",
       "      <td>2.954250</td>\n",
       "      <td>11.697413</td>\n",
       "      <td>0.332634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                53.0                10.0            23.255814   \n",
       "1                61.0                10.0            24.390244   \n",
       "2                69.0                10.0            26.220000   \n",
       "3                99.0                10.0            29.080000   \n",
       "4               129.0                10.0            35.350000   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
       "0         23.255814                  43                    1             1000   \n",
       "1         24.390244                  39                    1             2000   \n",
       "2         26.220000                  34                    1             3000   \n",
       "3         29.080000                  31                    1             4000   \n",
       "4         35.350000                  14                    1             5000   \n",
       "\n",
       "   agent_timesteps_total   done  episodes_total  ...  \\\n",
       "0                   1000  False              43  ...   \n",
       "1                   2000  False              82  ...   \n",
       "2                   3000  False             116  ...   \n",
       "3                   4000  False             147  ...   \n",
       "4                   5000  False             161  ...   \n",
       "\n",
       "   evaluation/sampler_perf/mean_inference_ms  \\\n",
       "0                                   1.175044   \n",
       "1                                   0.826491   \n",
       "2                                   0.819460   \n",
       "3                                   0.788918   \n",
       "4                                   0.775734   \n",
       "\n",
       "  evaluation/sampler_perf/mean_action_processing_ms  \\\n",
       "0                                          0.051374   \n",
       "1                                          0.047578   \n",
       "2                                          0.047618   \n",
       "3                                          0.046526   \n",
       "4                                          0.046113   \n",
       "\n",
       "  evaluation/sampler_perf/mean_env_wait_ms  \\\n",
       "0                                 0.055553   \n",
       "1                                 0.052969   \n",
       "2                                 0.053124   \n",
       "3                                 0.051812   \n",
       "4                                 0.051508   \n",
       "\n",
       "   evaluation/sampler_perf/mean_env_render_ms  \\\n",
       "0                                         0.0   \n",
       "1                                         0.0   \n",
       "2                                         0.0   \n",
       "3                                         0.0   \n",
       "4                                         0.0   \n",
       "\n",
       "                info/learner/default_policy/td_error  \\\n",
       "0  [ 0.1490928   0.80672693  0.07395267 -0.229648...   \n",
       "1  [-0.8847606  -0.14398265 -0.70628667 -0.102808...   \n",
       "2  [ 3.183545    0.0329237  -0.34508467 -0.671693...   \n",
       "3  [-5.6033611e-01 -1.8792248e-01  2.8856370e+00 ...   \n",
       "4  [-7.2457480e-01  4.5471525e-01  1.5333462e-01 ...   \n",
       "\n",
       "   info/learner/default_policy/learner_stats/cur_lr  \\\n",
       "0                                            0.0005   \n",
       "1                                            0.0005   \n",
       "2                                            0.0005   \n",
       "3                                            0.0005   \n",
       "4                                            0.0005   \n",
       "\n",
       "   info/learner/default_policy/learner_stats/mean_q  \\\n",
       "0                                          1.960149   \n",
       "1                                          3.598083   \n",
       "2                                          5.491560   \n",
       "3                                          7.154577   \n",
       "4                                          9.651964   \n",
       "\n",
       "  info/learner/default_policy/learner_stats/min_q  \\\n",
       "0                                        1.149093   \n",
       "1                                        1.941867   \n",
       "2                                        0.578560   \n",
       "3                                        1.618476   \n",
       "4                                        2.954250   \n",
       "\n",
       "  info/learner/default_policy/learner_stats/max_q  \\\n",
       "0                                        2.435233   \n",
       "1                                        4.605349   \n",
       "2                                        7.023629   \n",
       "3                                        9.023160   \n",
       "4                                       11.697413   \n",
       "\n",
       "   info/learner/default_policy/learner_stats/mean_td_error  \n",
       "0                                           0.055881        \n",
       "1                                          -0.034187        \n",
       "2                                           0.171092        \n",
       "3                                           0.620341        \n",
       "4                                           0.332634        \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# choose the path to your output logs\n",
    "data_path = '/mnt/data/Optiver_Realized_Volatility_Prediction_Challenge/DQN_2021-10-12_21-00-17/DQN_CartPole-v0_6747e_00000_0_2021-10-12_21-00-17/progress.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create dataframe with pertinent information and graph the episode reward mean against the episode  per iteration\n",
    "\n",
    "#episode_reward_mean = df['episode_reward_mean']\n",
    "episodes_this_iter = df['episodes_this_iter']\n",
    "episodes_total = df['episodes_total']\n",
    "episodes_reward_mean = df['evaluation/episode_reward_mean']\n",
    "episodes_reward_max = df['evaluation/episode_reward_max']\n",
    "episodes_reward_min = df['evaluation/episode_reward_min']\n",
    "\n",
    "df_episodes = pd.DataFrame(episodes_total)\n",
    "df_episodes['episodes_reward_mean'] = df['episode_reward_mean']\n",
    "df_episodes[\"episodes_reward_min\"] = df['episode_reward_min']\n",
    "df_episodes[\"episodes_reward_max\"] = df['episode_reward_max']\n",
    "\n",
    "df_episodes.plot.line()\n",
    "\n",
    "## here we see the total number of episodes has increased overall but in each iteration, fewer episodes are required \n",
    "## to achieve a higher reward.  We see the algorithm learning more quickly towards the end when it reaches its maximum\n",
    "## iterations and thus its best rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          1280        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_1[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 67,329\n",
      "Trainable params: 67,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy = trainer.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Environment and Optimized Hyperparameters\n",
    "\n",
    "lorum ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a custom environment; adapted from Ray documentation\n",
    "\n",
    "\"\"\"Example of a custom gym environment and model. Run this for a demo.\n",
    "\n",
    "This example shows:\n",
    "  - using a custom environment\n",
    "  - using a custom model\n",
    "  - using Tune for grid search\n",
    "\n",
    "You can visualize experiment results in ~/ray_results using TensorBoard.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import grid_search\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "run = 'DQN'\n",
    "framework = 'torch'\n",
    "as_test=\"store_true\"\n",
    "stop_iters = 50\n",
    "stop_timesteps = 100000\n",
    "stop_reward = 0.1\n",
    "\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Example of a custom env in which you have to walk down a corridor.\n",
    "\n",
    "    You can configure the length of the corridor via the env config.\"\"\"\n",
    "\n",
    "    def __init__(self, config: EnvContext):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = Discrete(2)\n",
    "        self.observation_space = Box(\n",
    "            0.0, self.end_pos, shape=(1, ), dtype=np.float32)\n",
    "        # Set the seed. This is only used for the final (reach goal) reward.\n",
    "        self.seed(config.worker_index * config.num_workers)\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos = 0\n",
    "        return [self.cur_pos]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1], action\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        done = self.cur_pos >= self.end_pos\n",
    "        # Produce a random reward when we reach the goal.\n",
    "        return [self.cur_pos], \\\n",
    "            random.random() * 2 if done else -0.1, done, {}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)\n",
    "\n",
    "\n",
    "class CustomModel(TFModelV2):\n",
    "    \"\"\"Example of a keras custom model that just delegates to an fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,\n",
    "                                          model_config, name)\n",
    "        self.model = FullyConnectedNetwork(obs_space, action_space,\n",
    "                                           num_outputs, model_config, name)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        return self.model.forward(input_dict, state, seq_lens)\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.model.value_function()\n",
    "\n",
    "\n",
    "class TorchCustomModel(TorchModelV2, nn.Module):\n",
    "    \"\"\"Example of a PyTorch custom model that just delegates to a fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.torch_sub_model = TorchFC(obs_space, action_space, num_outputs,\n",
    "                                       model_config, name)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        input_dict[\"obs\"] = input_dict[\"obs\"].float()\n",
    "        fc_out, _ = self.torch_sub_model(input_dict, state, seq_lens)\n",
    "        return fc_out, []\n",
    "\n",
    "    def value_function(self):\n",
    "        return torch.reshape(self.torch_sub_model.value_function(), [-1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is modified from the Ray documents.  The original can be found here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 4.0/30.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/6 CPUs, 0/0 GPUs, 0.0/15.76 GiB heap, 0.0/7.88 GiB objects<br>Result logdir: /home/ubuntu/ray_results/DQN<br>Number of trials: 3/3 (3 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.01  </td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">1e-06 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 15:17:25,075\tERROR syncer.py:73 -- Log sync requires rsync to be installed.\n",
      "\u001b[2m\u001b[36m(pid=1531)\u001b[0m 2021-10-20 15:17:27,664\tINFO dqn.py:141 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=1531)\u001b[0m 2021-10-20 15:17:27,664\tINFO trainer.py:758 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1530)\u001b[0m 2021-10-20 15:17:27,675\tINFO dqn.py:141 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=1530)\u001b[0m 2021-10-20 15:17:27,676\tINFO trainer.py:758 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1528)\u001b[0m 2021-10-20 15:17:27,738\tINFO dqn.py:141 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=1528)\u001b[0m 2021-10-20 15:17:27,738\tINFO trainer.py:758 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.4/30.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/6 CPUs, 0/0 GPUs, 0.0/15.76 GiB heap, 0.0/7.88 GiB objects<br>Result logdir: /home/ubuntu/ray_results/DQN<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.01  </td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">1e-06 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1531)\u001b[0m 2021-10-20 15:17:30,243\tWARNING deprecation.py:38 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=1531)\u001b[0m 2021-10-20 15:17:30,246\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=1530)\u001b[0m 2021-10-20 15:17:30,203\tWARNING deprecation.py:38 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=1530)\u001b[0m 2021-10-20 15:17:30,206\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=1528)\u001b[0m 2021-10-20 15:17:30,313\tWARNING deprecation.py:38 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=1528)\u001b[0m 2021-10-20 15:17:30,316\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_SimpleCorridor_d4611_00001:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-32\n",
      "  done: false\n",
      "  episode_len_mean: 29.176470588235293\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.127401990262979\n",
      "  episode_reward_mean: -1.7377393533805616\n",
      "  episode_reward_min: -14.898400599711488\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 34\n",
      "  experiment_id: ec2050c1f69849ec842acc5952b0ce0a\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 1000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0001\n",
      "          grad_gnorm: 4.211525917053223\n",
      "          max_q: 0.0\n",
      "          mean_q: -0.2889972925186157\n",
      "          min_q: -0.8655588626861572\n",
      "        mean_td_error: -0.18787328898906708\n",
      "        model: {}\n",
      "        td_error: \"[-0.7069056   0.09658475  0.09904046  0.08101392  0.09904046 -1.396606\\n\\\n",
      "          \\ -0.6829462   0.09904046 -0.7069056   0.17686296  0.1         0.17686296\\n\\\n",
      "          \\  0.1        -0.7069056  -0.68030536  0.09658475  0.17686296  0.08101392\\n\\\n",
      "          \\ -0.6829462   0.1         0.08101392  0.08101392 -0.68030536 -0.68030536\\n\\\n",
      "          \\  0.1         0.17686296  0.08101392 -0.68030536 -0.7069056   0.09658475\\n\\\n",
      "          \\  0.1         0.1       ]\"\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 32\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.0\n",
      "    ram_util_percent: 17.7\n",
      "  pid: 1531\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.055894985065593594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0343908677687059\n",
      "    mean_inference_ms: 1.1036169755232563\n",
      "    mean_raw_obs_processing_ms: 0.13167064983051616\n",
      "  time_since_restore: 1.8355402946472168\n",
      "  time_this_iter_s: 1.8355402946472168\n",
      "  time_total_s: 1.8355402946472168\n",
      "  timers:\n",
      "    learn_throughput: 2265.469\n",
      "    learn_time_ms: 14.125\n",
      "    load_throughput: 68618.47\n",
      "    load_time_ms: 0.466\n",
      "    update_time_ms: 2.007\n",
      "  timestamp: 1634743052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: d4611_00001\n",
      "  \n",
      "Result for DQN_SimpleCorridor_d4611_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-32\n",
      "  done: false\n",
      "  episode_len_mean: 22.6046511627907\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.344377124939775\n",
      "  episode_reward_mean: -1.1232731139468826\n",
      "  episode_reward_min: -5.949108278013073\n",
      "  episodes_this_iter: 43\n",
      "  episodes_total: 43\n",
      "  experiment_id: c51033ebd08941aaa937deaa8e824512\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 1000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.01\n",
      "          grad_gnorm: 5.4764299392700195\n",
      "          max_q: 1.4952926635742188\n",
      "          mean_q: 0.3063879609107971\n",
      "          min_q: -0.3403472900390625\n",
      "        mean_td_error: -0.4865272343158722\n",
      "        model: {}\n",
      "        td_error: \"[-0.17125979  0.1        -1.6950822   0.09293211 -0.17125979  0.1\\n\\\n",
      "          \\ -0.05144322 -0.17125979 -1.1910671  -0.17125979 -1.1910671  -0.17125979\\n\\\n",
      "          \\ -1.7176691  -0.17125979 -0.05144322  0.1         0.1119045  -1.1910671\\n\\\n",
      "          \\  0.1119045   0.1        -1.7176691  -1.5236354  -0.05144322  0.1119045\\n\\\n",
      "          \\ -1.5236354  -0.05144322  0.1         1.3075736  -1.6950822   0.09293211\\n\\\n",
      "          \\ -1.6950822  -1.5236354 ]\"\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 32\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.100000000000005\n",
      "    ram_util_percent: 17.7\n",
      "  pid: 1530\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.055754458630358904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.035021807644869785\n",
      "    mean_inference_ms: 1.1317144026170363\n",
      "    mean_raw_obs_processing_ms: 0.13688370421692567\n",
      "  time_since_restore: 1.8933193683624268\n",
      "  time_this_iter_s: 1.8933193683624268\n",
      "  time_total_s: 1.8933193683624268\n",
      "  timers:\n",
      "    learn_throughput: 3297.733\n",
      "    learn_time_ms: 9.704\n",
      "    load_throughput: 139374.588\n",
      "    load_time_ms: 0.23\n",
      "    update_time_ms: 1.487\n",
      "  timestamp: 1634743052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: d4611_00000\n",
      "  \n",
      "Result for DQN_SimpleCorridor_d4611_00002:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-32\n",
      "  done: false\n",
      "  episode_len_mean: 27.444444444444443\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.344377124939775\n",
      "  episode_reward_mean: -1.600760530791577\n",
      "  episode_reward_min: -10.66159225035555\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 36\n",
      "  experiment_id: 4e02c389b1074b698a957814f1dc1e0f\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 1000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 1.0e-06\n",
      "          grad_gnorm: 3.7385597229003906\n",
      "          max_q: 0.3129672408103943\n",
      "          mean_q: -0.10212911665439606\n",
      "          min_q: -0.7328299283981323\n",
      "        mean_td_error: -0.20149430632591248\n",
      "        model: {}\n",
      "        td_error: \"[-0.9426675   0.10299295 -0.9413836  -0.5852668  -0.19615781 -0.19615781\\n\\\n",
      "          \\ -0.19615781 -0.19615781  0.09015071 -0.9426675   0.1         0.1\\n -0.19615781\\\n",
      "          \\  0.09015071  0.1         0.1        -0.5852668   0.1\\n  0.1        -0.19615781\\\n",
      "          \\  0.10299295 -0.19615781  0.10299295 -0.19615781\\n  0.1         0.09015071\\\n",
      "          \\  0.09015071  0.1        -0.9426675  -0.5852668\\n  0.1022822  -0.92523295]\"\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 32\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.03333333333333\n",
      "    ram_util_percent: 17.733333333333334\n",
      "  pid: 1528\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05487652568074017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03411862757298854\n",
      "    mean_inference_ms: 1.0862383809123004\n",
      "    mean_raw_obs_processing_ms: 0.13262765867250426\n",
      "  time_since_restore: 1.815608024597168\n",
      "  time_this_iter_s: 1.815608024597168\n",
      "  time_total_s: 1.815608024597168\n",
      "  timers:\n",
      "    learn_throughput: 2867.9\n",
      "    learn_time_ms: 11.158\n",
      "    load_throughput: 114912.438\n",
      "    load_time_ms: 0.278\n",
      "    update_time_ms: 1.337\n",
      "  timestamp: 1634743052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: d4611_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.5/30.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/6 CPUs, 0/0 GPUs, 0.0/15.76 GiB heap, 0.0/7.88 GiB objects<br>Result logdir: /home/ubuntu/ray_results/DQN<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00000</td><td>RUNNING </td><td>10.0.33.251:1530</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.89332</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-1.12327</td><td style=\"text-align: right;\">             1.34438</td><td style=\"text-align: right;\">            -5.94911</td><td style=\"text-align: right;\">           22.6047</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00001</td><td>RUNNING </td><td>10.0.33.251:1531</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.83554</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-1.73774</td><td style=\"text-align: right;\">             1.1274 </td><td style=\"text-align: right;\">           -14.8984 </td><td style=\"text-align: right;\">           29.1765</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00002</td><td>RUNNING </td><td>10.0.33.251:1528</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.81561</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-1.60076</td><td style=\"text-align: right;\">             1.34438</td><td style=\"text-align: right;\">           -10.6616 </td><td style=\"text-align: right;\">           27.4444</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_SimpleCorridor_d4611_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-38\n",
      "  done: false\n",
      "  episode_len_mean: 24.036585365853657\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.344377124939775\n",
      "  episode_reward_mean: -1.2407014000799201\n",
      "  episode_reward_min: -9.671150962321768\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 82\n",
      "  experiment_id: c51033ebd08941aaa937deaa8e824512\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 1504\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.01\n",
      "          grad_gnorm: 0.6956409811973572\n",
      "          max_q: 2.1088180541992188\n",
      "          mean_q: 2.0795557498931885\n",
      "          min_q: 2.013923168182373\n",
      "        mean_td_error: 0.7740449905395508\n",
      "        model: {}\n",
      "        td_error: \"[-0.1075666   0.11240399  1.9300771   0.0177356   1.8579483   1.9300771\\n\\\n",
      "          \\  0.0177356  -0.1075666  -0.2536924  -0.1075666   0.11240399 -0.20425177\\n\\\n",
      "          \\  1.9300771   0.0177356   1.9300771  -0.1075666  -0.22847724  0.11240399\\n\\\n",
      "          \\ -0.2536924  -0.1075666   1.8579483   1.8579483   1.8579483   1.9300771\\n\\\n",
      "          \\  0.5570432   0.0177356   1.8579483   1.9300771   1.8579483   0.11240399\\n\\\n",
      "          \\  1.9300771   0.5115565 ]\"\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 8032\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 8032\n",
      "    num_target_updates: 2\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.41111111111111\n",
      "    ram_util_percent: 17.866666666666667\n",
      "  pid: 1530\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.056971355292217925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03610802845171448\n",
      "    mean_inference_ms: 1.174734822977087\n",
      "    mean_raw_obs_processing_ms: 0.14375649972049556\n",
      "  time_since_restore: 7.932153224945068\n",
      "  time_this_iter_s: 6.038833856582642\n",
      "  time_total_s: 7.932153224945068\n",
      "  timers:\n",
      "    learn_throughput: 3919.613\n",
      "    learn_time_ms: 8.164\n",
      "    load_throughput: 112721.7\n",
      "    load_time_ms: 0.284\n",
      "    update_time_ms: 1.252\n",
      "  timestamp: 1634743058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: d4611_00000\n",
      "  \n",
      "Result for DQN_SimpleCorridor_d4611_00001:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-38\n",
      "  done: false\n",
      "  episode_len_mean: 24.395061728395063\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3663754346193477\n",
      "  episode_reward_mean: -1.2820825723093556\n",
      "  episode_reward_min: -14.898400599711488\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 81\n",
      "  experiment_id: ec2050c1f69849ec842acc5952b0ce0a\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 1504\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0001\n",
      "          grad_gnorm: 0.17963707447052002\n",
      "          max_q: 0.5844786763191223\n",
      "          mean_q: 0.1898784190416336\n",
      "          min_q: -0.2789223790168762\n",
      "        mean_td_error: 0.020961496978998184\n",
      "        model: {}\n",
      "        td_error: \"[ 0.04281357 -0.1925793   0.23443818  0.04281357  0.04281357  0.15391484\\n\\\n",
      "          \\  0.15391484  0.04281357  0.04281357  0.15391484 -0.16648301 -0.37772495\\n\\\n",
      "          \\  0.15391484 -0.16648301 -0.0120613  -0.0120613   0.15391484  0.15391484\\n\\\n",
      "          \\  0.04281357  0.18917882  0.18917882 -0.04826234 -0.16648301 -0.05299056\\n\\\n",
      "          \\ -0.16648301  0.04281357 -0.05299056  0.04281357 -0.04826234  0.15391484\\n\\\n",
      "          \\  0.15391484 -0.05299056]\"\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 8032\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 8032\n",
      "    num_target_updates: 2\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.46666666666667\n",
      "    ram_util_percent: 17.844444444444445\n",
      "  pid: 1531\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0577711660545796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0357155353897985\n",
      "    mean_inference_ms: 1.16475339805641\n",
      "    mean_raw_obs_processing_ms: 0.1411734464119093\n",
      "  time_since_restore: 7.934174537658691\n",
      "  time_this_iter_s: 6.098634243011475\n",
      "  time_total_s: 7.934174537658691\n",
      "  timers:\n",
      "    learn_throughput: 3982.781\n",
      "    learn_time_ms: 8.035\n",
      "    load_throughput: 100771.625\n",
      "    load_time_ms: 0.318\n",
      "    update_time_ms: 1.258\n",
      "  timestamp: 1634743058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: d4611_00001\n",
      "  \n",
      "Result for DQN_SimpleCorridor_d4611_00002:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-38\n",
      "  done: false\n",
      "  episode_len_mean: 22.363636363636363\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.503915303470864\n",
      "  episode_reward_mean: -1.087512082981759\n",
      "  episode_reward_min: -10.66159225035555\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 88\n",
      "  experiment_id: 4e02c389b1074b698a957814f1dc1e0f\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 1504\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 1.0e-06\n",
      "          grad_gnorm: 0.26544517278671265\n",
      "          max_q: 0.5344371795654297\n",
      "          mean_q: 0.13946214318275452\n",
      "          min_q: -0.01840883493423462\n",
      "        mean_td_error: -0.16169165074825287\n",
      "        model: {}\n",
      "        td_error: \"[ 0.1771476  -0.31460512 -0.2874662  -0.2874662  -0.2874662   0.07906013\\n\\\n",
      "          \\  0.18159914 -0.31460512 -0.31460512 -0.98745835 -0.31460512 -0.31460512\\n\\\n",
      "          \\  0.10112847 -0.31460512 -0.31460512 -0.31460512  0.18159914 -0.31460512\\n\\\n",
      "          \\  0.07906013 -0.30779564 -0.31460512 -0.2874662  -0.2874662   0.1771476\\n\\\n",
      "          \\  0.10112847  0.10112847  0.14268106  0.1771476  -0.31460512 -0.31460512\\n\\\n",
      "          \\ -0.30779564  0.14268106]\"\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 8032\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 8032\n",
      "    num_target_updates: 2\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.56666666666666\n",
      "    ram_util_percent: 17.855555555555558\n",
      "  pid: 1528\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05696958644663544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.035441927867038765\n",
      "    mean_inference_ms: 1.1536165598630517\n",
      "    mean_raw_obs_processing_ms: 0.1428816324768757\n",
      "  time_since_restore: 7.911417245864868\n",
      "  time_this_iter_s: 6.0958092212677\n",
      "  time_total_s: 7.911417245864868\n",
      "  timers:\n",
      "    learn_throughput: 3677.087\n",
      "    learn_time_ms: 8.703\n",
      "    load_throughput: 132403.796\n",
      "    load_time_ms: 0.242\n",
      "    update_time_ms: 1.177\n",
      "  timestamp: 1634743058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: d4611_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.5/30.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/6 CPUs, 0/0 GPUs, 0.0/15.76 GiB heap, 0.0/7.88 GiB objects<br>Result logdir: /home/ubuntu/ray_results/DQN<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00000</td><td>RUNNING </td><td>10.0.33.251:1530</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.93215</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1.2407 </td><td style=\"text-align: right;\">             1.34438</td><td style=\"text-align: right;\">            -9.67115</td><td style=\"text-align: right;\">           24.0366</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00001</td><td>RUNNING </td><td>10.0.33.251:1531</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.93417</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1.28208</td><td style=\"text-align: right;\">             1.36638</td><td style=\"text-align: right;\">           -14.8984 </td><td style=\"text-align: right;\">           24.3951</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00002</td><td>RUNNING </td><td>10.0.33.251:1528</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.91142</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1.08751</td><td style=\"text-align: right;\">             1.50392</td><td style=\"text-align: right;\">           -10.6616 </td><td style=\"text-align: right;\">           22.3636</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_SimpleCorridor_d4611_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-44\n",
      "  done: false\n",
      "  episode_len_mean: 29.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.344377124939775\n",
      "  episode_reward_mean: -1.8096768033134043\n",
      "  episode_reward_min: -21.73969553842974\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 100\n",
      "  experiment_id: c51033ebd08941aaa937deaa8e824512\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 2512\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.01\n",
      "          grad_gnorm: 0.07985950261354446\n",
      "          max_q: 1.3866349458694458\n",
      "          mean_q: 1.3195481300354004\n",
      "          min_q: 1.2759565114974976\n",
      "        mean_td_error: 0.0794672966003418\n",
      "        model: {}\n",
      "        td_error: \"[-0.04974437  0.0242008   0.0242008   0.02524781  1.0475758   0.9215346\\n\\\n",
      "          \\ -0.0494436   0.0242008   0.0242008   0.0250901  -0.04974437 -0.01119113\\n\\\n",
      "          \\ -0.0494436  -0.0494436   0.0242008  -0.0494436   0.277233   -0.0494436\\n\\\n",
      "          \\  0.0242008  -0.04973519  0.00635684 -0.0494436  -0.01119113 -0.0494436\\n\\\n",
      "          \\  0.0250901   0.0242008   0.91438913 -0.26508915 -0.01119113 -0.0494436\\n\\\n",
      "          \\  0.0242008  -0.04973519]\"\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 16032\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 16032\n",
      "    num_target_updates: 4\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.72222222222222\n",
      "    ram_util_percent: 17.9\n",
      "  pid: 1530\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.057403819188703215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.036423285449251185\n",
      "    mean_inference_ms: 1.188560726925838\n",
      "    mean_raw_obs_processing_ms: 0.14557498425141727\n",
      "  time_since_restore: 14.012385606765747\n",
      "  time_this_iter_s: 6.080232381820679\n",
      "  time_total_s: 14.012385606765747\n",
      "  timers:\n",
      "    learn_throughput: 3849.431\n",
      "    learn_time_ms: 8.313\n",
      "    load_throughput: 102935.599\n",
      "    load_time_ms: 0.311\n",
      "    update_time_ms: 1.367\n",
      "  timestamp: 1634743064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: d4611_00000\n",
      "  \n",
      "Result for DQN_SimpleCorridor_d4611_00001:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-44\n",
      "  done: false\n",
      "  episode_len_mean: 20.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5197866344485815\n",
      "  episode_reward_mean: -0.8860141687974117\n",
      "  episode_reward_min: -5.796084696529125\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 133\n",
      "  experiment_id: ec2050c1f69849ec842acc5952b0ce0a\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 2512\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0001\n",
      "          grad_gnorm: 0.02737232856452465\n",
      "          max_q: 0.833837628364563\n",
      "          mean_q: 0.502717912197113\n",
      "          min_q: 0.09033791720867157\n",
      "        mean_td_error: -0.0020278580486774445\n",
      "        model: {}\n",
      "        td_error: \"[ 4.3322891e-04  9.6784651e-02  2.8994977e-03  4.4761784e-02\\n  1.2571812e-01\\\n",
      "          \\  4.4761784e-02 -7.3298991e-02 -1.2350863e-01\\n -1.5648144e-01 -7.3298991e-02\\\n",
      "          \\  5.9800023e-01  7.1155131e-02\\n  4.3322891e-04  9.6784651e-02  4.4761784e-02\\\n",
      "          \\  6.6736937e-01\\n  4.4761784e-02 -8.8006067e-01  9.6784651e-02 -7.3298991e-02\\n\\\n",
      "          \\  9.6784651e-02 -1.2350863e-01 -1.2350863e-01  7.1155131e-02\\n  4.4761784e-02\\\n",
      "          \\ -3.9041913e-01  7.1155131e-02 -7.3298991e-02\\n -1.2350863e-01 -7.3298991e-02\\\n",
      "          \\  2.8994977e-03  4.3322891e-04]\"\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 16032\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 16032\n",
      "    num_target_updates: 4\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.711111111111116\n",
      "    ram_util_percent: 17.9\n",
      "  pid: 1531\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05947174096430073\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.036914213315353994\n",
      "    mean_inference_ms: 1.2206650572497189\n",
      "    mean_raw_obs_processing_ms: 0.14965977677086936\n",
      "  time_since_restore: 13.990536451339722\n",
      "  time_this_iter_s: 6.05636191368103\n",
      "  time_total_s: 13.990536451339722\n",
      "  timers:\n",
      "    learn_throughput: 3887.653\n",
      "    learn_time_ms: 8.231\n",
      "    load_throughput: 123361.882\n",
      "    load_time_ms: 0.259\n",
      "    update_time_ms: 1.219\n",
      "  timestamp: 1634743064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: d4611_00001\n",
      "  \n",
      "Result for DQN_SimpleCorridor_d4611_00002:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-44\n",
      "  done: false\n",
      "  episode_len_mean: 15.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.503915303470864\n",
      "  episode_reward_mean: -0.39674973419229553\n",
      "  episode_reward_min: -4.011014445801055\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 160\n",
      "  experiment_id: 4e02c389b1074b698a957814f1dc1e0f\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 2512\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 1.0e-06\n",
      "          grad_gnorm: 0.145880788564682\n",
      "          max_q: 0.6452467441558838\n",
      "          mean_q: 0.2758179306983948\n",
      "          min_q: 0.006907095666974783\n",
      "        mean_td_error: -0.0810031071305275\n",
      "        model: {}\n",
      "        td_error: \"[ 0.09685178 -0.41903514 -0.41903514 -0.171832    0.1571739  -0.171832\\n\\\n",
      "          \\  0.63382846 -0.171832    0.10199574 -0.41903514  0.26867932 -0.2077131\\n\\\n",
      "          \\ -0.171832   -0.41903514 -0.41903514  0.1509156  -0.41903514  0.1571739\\n\\\n",
      "          \\ -0.9638922   0.26867932  0.09685178  0.26867932 -0.171832    0.09685178\\n\\\n",
      "          \\  0.26867932  0.10199574  0.10199574 -0.171832   -0.21194965 -0.171832\\n\\\n",
      "          \\  0.1571739  -0.41903514]\"\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 16032\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 16032\n",
      "    num_target_updates: 4\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.67777777777778\n",
      "    ram_util_percent: 17.9\n",
      "  pid: 1528\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05920746503864468\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03689353527642795\n",
      "    mean_inference_ms: 1.2288180483216764\n",
      "    mean_raw_obs_processing_ms: 0.15485226738929414\n",
      "  time_since_restore: 14.055476427078247\n",
      "  time_this_iter_s: 6.144059181213379\n",
      "  time_total_s: 14.055476427078247\n",
      "  timers:\n",
      "    learn_throughput: 4019.771\n",
      "    learn_time_ms: 7.961\n",
      "    load_throughput: 119997.969\n",
      "    load_time_ms: 0.267\n",
      "    update_time_ms: 1.251\n",
      "  timestamp: 1634743064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: d4611_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.5/30.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/6 CPUs, 0/0 GPUs, 0.0/15.76 GiB heap, 0.0/7.88 GiB objects<br>Result logdir: /home/ubuntu/ray_results/DQN<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00000</td><td>RUNNING </td><td>10.0.33.251:1530</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         14.0124</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-1.80968 </td><td style=\"text-align: right;\">             1.34438</td><td style=\"text-align: right;\">           -21.7397 </td><td style=\"text-align: right;\">             29.5 </td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00001</td><td>RUNNING </td><td>10.0.33.251:1531</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         13.9905</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-0.886014</td><td style=\"text-align: right;\">             1.51979</td><td style=\"text-align: right;\">            -5.79608</td><td style=\"text-align: right;\">             20.08</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00002</td><td>RUNNING </td><td>10.0.33.251:1528</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         14.0555</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-0.39675 </td><td style=\"text-align: right;\">             1.50392</td><td style=\"text-align: right;\">            -4.01101</td><td style=\"text-align: right;\">             15.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_SimpleCorridor_d4611_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 36.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.344377124939775\n",
      "  episode_reward_mean: -2.48831502876439\n",
      "  episode_reward_min: -23.70278810075405\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 114\n",
      "  experiment_id: c51033ebd08941aaa937deaa8e824512\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 3520\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.01\n",
      "          grad_gnorm: 0.01360953040421009\n",
      "          max_q: 1.0786311626434326\n",
      "          mean_q: 1.0733864307403564\n",
      "          min_q: 1.0709255933761597\n",
      "        mean_td_error: -0.0004575047641992569\n",
      "        model: {}\n",
      "        td_error: \"[-0.01034176  0.00548303 -0.7485843  -0.01788795  1.0595075  -0.01787496\\n\\\n",
      "          \\ -0.01034176 -0.01034176  0.92947197 -0.45096982  0.00548303 -0.01787496\\n\\\n",
      "          \\  0.70935774 -0.3893788   0.00548303 -0.01787496 -0.01217246 -0.01788795\\n\\\n",
      "          \\  0.52509683  0.00548303 -0.01788819 -0.01787496  0.5603376  -0.01232696\\n\\\n",
      "          \\ -0.501963   -0.01034176 -0.01788795 -0.01034176 -0.01787496  0.00548303\\n\\\n",
      "          \\ -0.78208756 -0.7157084 ]\"\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 24032\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 24032\n",
      "    num_target_updates: 6\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.1375\n",
      "    ram_util_percent: 17.95\n",
      "  pid: 1530\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05793763948868014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03683930141923497\n",
      "    mean_inference_ms: 1.2064292761926267\n",
      "    mean_raw_obs_processing_ms: 0.1479046403814088\n",
      "  time_since_restore: 20.00777578353882\n",
      "  time_this_iter_s: 5.995390176773071\n",
      "  time_total_s: 20.00777578353882\n",
      "  timers:\n",
      "    learn_throughput: 3323.233\n",
      "    learn_time_ms: 9.629\n",
      "    load_throughput: 107074.374\n",
      "    load_time_ms: 0.299\n",
      "    update_time_ms: 1.343\n",
      "  timestamp: 1634743070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: d4611_00000\n",
      "  \n",
      "Result for DQN_SimpleCorridor_d4611_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 14.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.335112101896351\n",
      "  episode_reward_mean: -0.3417931474770017\n",
      "  episode_reward_min: -4.161171778774401\n",
      "  episodes_this_iter: 82\n",
      "  episodes_total: 215\n",
      "  experiment_id: ec2050c1f69849ec842acc5952b0ce0a\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 3520\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0001\n",
      "          grad_gnorm: 0.06543856859207153\n",
      "          max_q: 0.9188047647476196\n",
      "          mean_q: 0.6951513290405273\n",
      "          min_q: 0.34436744451522827\n",
      "        mean_td_error: 0.0343785434961319\n",
      "        model: {}\n",
      "        td_error: \"[-1.7221808e-02  1.7083734e-01  1.5062290e-01 -1.8864036e-02\\n  1.5062290e-01\\\n",
      "          \\  9.9389613e-02  1.5062290e-01 -2.6401389e-01\\n  9.9389613e-02 -5.3323299e-02\\\n",
      "          \\ -5.3323299e-02  1.7083734e-01\\n  1.5062290e-01 -6.1190128e-04 -5.3323299e-02\\\n",
      "          \\  2.7578175e-02\\n -2.2990775e-01 -5.3323299e-02 -5.3323299e-02  7.5233650e-01\\n\\\n",
      "          \\ -5.3323299e-02 -1.8894672e-04 -1.7221808e-02 -3.7349880e-02\\n  2.7578175e-02\\\n",
      "          \\ -1.7221808e-02  1.7083734e-01 -3.3079946e-01\\n -6.1190128e-04 -1.7221808e-02\\\n",
      "          \\  9.9389613e-02  1.5062290e-01]\"\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 24032\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 24032\n",
      "    num_target_updates: 6\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.1625\n",
      "    ram_util_percent: 17.95\n",
      "  pid: 1531\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06029239734395171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.037451792903787105\n",
      "    mean_inference_ms: 1.2530764294678682\n",
      "    mean_raw_obs_processing_ms: 0.15531272377890437\n",
      "  time_since_restore: 20.1158344745636\n",
      "  time_this_iter_s: 6.125298023223877\n",
      "  time_total_s: 20.1158344745636\n",
      "  timers:\n",
      "    learn_throughput: 3827.487\n",
      "    learn_time_ms: 8.361\n",
      "    load_throughput: 73871.83\n",
      "    load_time_ms: 0.433\n",
      "    update_time_ms: 1.312\n",
      "  timestamp: 1634743070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: d4611_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.5/30.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/6 CPUs, 0/0 GPUs, 0.0/15.76 GiB heap, 0.0/7.88 GiB objects<br>Result logdir: /home/ubuntu/ray_results/DQN<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00000</td><td>RUNNING </td><td>10.0.33.251:1530</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         20.0078</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-2.48832 </td><td style=\"text-align: right;\">             1.34438</td><td style=\"text-align: right;\">           -23.7028 </td><td style=\"text-align: right;\">             36.28</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00001</td><td>RUNNING </td><td>10.0.33.251:1531</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         20.1158</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-0.341793</td><td style=\"text-align: right;\">             1.33511</td><td style=\"text-align: right;\">            -4.16117</td><td style=\"text-align: right;\">             14.06</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00002</td><td>RUNNING </td><td>10.0.33.251:1528</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         14.0555</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-0.39675 </td><td style=\"text-align: right;\">             1.50392</td><td style=\"text-align: right;\">            -4.01101</td><td style=\"text-align: right;\">             15.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_SimpleCorridor_d4611_00002:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 11.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.559905771780154\n",
      "  episode_reward_mean: -0.11272404693280423\n",
      "  episode_reward_min: -2.905406226924266\n",
      "  episodes_this_iter: 88\n",
      "  episodes_total: 248\n",
      "  experiment_id: 4e02c389b1074b698a957814f1dc1e0f\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 3520\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 1.0e-06\n",
      "          grad_gnorm: 0.2806655764579773\n",
      "          max_q: 0.6854830384254456\n",
      "          mean_q: 0.3453783690929413\n",
      "          min_q: 0.010297674685716629\n",
      "        mean_td_error: -0.19992183148860931\n",
      "        model: {}\n",
      "        td_error: \"[-1.049629   -1.2808924   0.32342976 -0.46979892 -0.15586552  0.12229121\\n\\\n",
      "          \\ -0.61152977 -0.46979892  0.11551744  0.06118977  0.09037156  0.32342976\\n\\\n",
      "          \\ -0.20029235  0.11551744  0.06118977 -0.46979892 -0.46979892  0.06118977\\n\\\n",
      "          \\ -1.1914353  -0.20029235 -0.46979892 -0.46979892 -0.20029235  0.09037156\\n\\\n",
      "          \\  0.06118977  0.32342976 -0.20604494  0.06118977 -0.46979892 -0.46979892\\n\\\n",
      "          \\  0.32342976  0.32342976]\"\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 24032\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 24032\n",
      "    num_target_updates: 6\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.162499999999994\n",
      "    ram_util_percent: 17.95\n",
      "  pid: 1528\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.059995478146928766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.037420860581973754\n",
      "    mean_inference_ms: 1.2582651012615895\n",
      "    mean_raw_obs_processing_ms: 0.16066193087497957\n",
      "  time_since_restore: 20.123842239379883\n",
      "  time_this_iter_s: 6.068365812301636\n",
      "  time_total_s: 20.123842239379883\n",
      "  timers:\n",
      "    learn_throughput: 4288.326\n",
      "    learn_time_ms: 7.462\n",
      "    load_throughput: 136109.652\n",
      "    load_time_ms: 0.235\n",
      "    update_time_ms: 1.173\n",
      "  timestamp: 1634743070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: d4611_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.5/30.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/6 CPUs, 0/0 GPUs, 0.0/15.76 GiB heap, 0.0/7.88 GiB objects<br>Result logdir: /home/ubuntu/ray_results/DQN<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00000</td><td>RUNNING </td><td>10.0.33.251:1530</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         20.0078</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-2.48832 </td><td style=\"text-align: right;\">             1.34438</td><td style=\"text-align: right;\">           -23.7028 </td><td style=\"text-align: right;\">             36.28</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00001</td><td>RUNNING </td><td>10.0.33.251:1531</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         20.1158</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-0.341793</td><td style=\"text-align: right;\">             1.33511</td><td style=\"text-align: right;\">            -4.16117</td><td style=\"text-align: right;\">             14.06</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00002</td><td>RUNNING </td><td>10.0.33.251:1528</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         20.1238</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-0.112724</td><td style=\"text-align: right;\">             1.55991</td><td style=\"text-align: right;\">            -2.90541</td><td style=\"text-align: right;\">             11.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_SimpleCorridor_d4611_00002:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-56\n",
      "  done: true\n",
      "  episode_len_mean: 10.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5447075373890278\n",
      "  episode_reward_mean: 0.12121424697288948\n",
      "  episode_reward_min: -2.79224868424489\n",
      "  episodes_this_iter: 95\n",
      "  episodes_total: 343\n",
      "  experiment_id: 4e02c389b1074b698a957814f1dc1e0f\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 4528\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 1.0e-06\n",
      "          grad_gnorm: 0.10727444291114807\n",
      "          max_q: 0.7289170622825623\n",
      "          mean_q: 0.38893935084342957\n",
      "          min_q: 0.01334238052368164\n",
      "        mean_td_error: -0.12062804400920868\n",
      "        model: {}\n",
      "        td_error: \"[ 0.0817014   0.11599994 -0.20470387  0.05417353 -0.2108767  -0.49174297\\n\\\n",
      "          \\  0.0817014  -0.49174297 -0.49174297  0.11599994  0.1234864  -0.2108767\\n\\\n",
      "          \\ -0.49174297 -1.2186334  -0.15394658  0.11599994  0.11599994 -0.49174297\\n\\\n",
      "          \\  0.5005358   0.1234864  -0.2108767  -0.15394658 -0.49174297  0.33729112\\n\\\n",
      "          \\ -0.2108767  -0.09971094  0.33729112  0.1234864  -0.15394658 -0.15394658\\n\\\n",
      "          \\ -0.49174297  0.33729112]\"\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 32032\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 32032\n",
      "    num_target_updates: 8\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.888888888888886\n",
      "    ram_util_percent: 18.0\n",
      "  pid: 1528\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06040895474185819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03780314638137093\n",
      "    mean_inference_ms: 1.2753225813593962\n",
      "    mean_raw_obs_processing_ms: 0.1640486450819464\n",
      "  time_since_restore: 26.23746156692505\n",
      "  time_this_iter_s: 6.113619327545166\n",
      "  time_total_s: 26.23746156692505\n",
      "  timers:\n",
      "    learn_throughput: 3856.587\n",
      "    learn_time_ms: 8.297\n",
      "    load_throughput: 112920.855\n",
      "    load_time_ms: 0.283\n",
      "    update_time_ms: 1.192\n",
      "  timestamp: 1634743076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: d4611_00002\n",
      "  \n",
      "Result for DQN_SimpleCorridor_d4611_00001:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-56\n",
      "  done: true\n",
      "  episode_len_mean: 9.58095238095238\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.552624001660997\n",
      "  episode_reward_mean: 0.17311507620490832\n",
      "  episode_reward_min: -1.700874120222094\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 320\n",
      "  experiment_id: ec2050c1f69849ec842acc5952b0ce0a\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 4528\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0001\n",
      "          grad_gnorm: 0.04202564060688019\n",
      "          max_q: 0.8175525665283203\n",
      "          mean_q: 0.6964815855026245\n",
      "          min_q: 0.3614810109138489\n",
      "        mean_td_error: -0.06907397508621216\n",
      "        model: {}\n",
      "        td_error: \"[ 3.3490777e-02 -3.0516386e-02  7.0595324e-02 -9.7151458e-02\\n  3.3490777e-02\\\n",
      "          \\ -7.8493893e-02 -7.8493893e-02  4.8681498e-02\\n -7.8493893e-02  4.8681498e-02\\\n",
      "          \\ -7.8493893e-02  3.3490777e-02\\n  4.8681498e-02 -8.5760343e-01 -9.8929930e-01\\\n",
      "          \\  4.8681498e-02\\n -2.4474621e-02  3.2295880e-01 -3.0516386e-02  7.0595324e-02\\n\\\n",
      "          \\  4.8681498e-02 -7.8493893e-02 -7.8493893e-02  3.5049325e-01\\n  7.0595324e-02\\\n",
      "          \\ -7.8493893e-02  2.0630682e-01  4.8681498e-02\\n -8.6349249e-04  3.3490777e-02\\\n",
      "          \\ -1.0695879e+00 -7.8493893e-02]\"\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 32032\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 32032\n",
      "    num_target_updates: 8\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.900000000000006\n",
      "    ram_util_percent: 18.0\n",
      "  pid: 1531\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06075950413554986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.037830654465420395\n",
      "    mean_inference_ms: 1.2745572147167246\n",
      "    mean_raw_obs_processing_ms: 0.15986115902429868\n",
      "  time_since_restore: 26.309834480285645\n",
      "  time_this_iter_s: 6.194000005722046\n",
      "  time_total_s: 26.309834480285645\n",
      "  timers:\n",
      "    learn_throughput: 3975.055\n",
      "    learn_time_ms: 8.05\n",
      "    load_throughput: 124540.9\n",
      "    load_time_ms: 0.257\n",
      "    update_time_ms: 1.232\n",
      "  timestamp: 1634743076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: d4611_00001\n",
      "  \n",
      "Result for DQN_SimpleCorridor_d4611_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-17-56\n",
      "  done: false\n",
      "  episode_len_mean: 20.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4184454563014226\n",
      "  episode_reward_mean: -0.9934237404356412\n",
      "  episode_reward_min: -23.70278810075405\n",
      "  episodes_this_iter: 86\n",
      "  episodes_total: 200\n",
      "  experiment_id: c51033ebd08941aaa937deaa8e824512\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 4528\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.01\n",
      "          grad_gnorm: 0.0026374084409326315\n",
      "          max_q: 0.8805363178253174\n",
      "          mean_q: 0.8632973432540894\n",
      "          min_q: 0.8205493092536926\n",
      "        mean_td_error: 0.009556490927934647\n",
      "        model: {}\n",
      "        td_error: \"[ 0.01499164  0.0149917   0.0149917  -0.03814662  0.01499164 -0.03814662\\n\\\n",
      "          \\  0.0149917   0.28943402  0.192882    0.01499164 -0.01711446  0.66467124\\n\\\n",
      "          \\ -0.02396971  0.01499164 -0.02396327 -0.27938038 -0.02396971 -0.67426056\\n\\\n",
      "          \\ -0.03814662 -0.03814662  0.01499414 -0.02396315 -0.02396971  0.01499414\\n\\\n",
      "          \\  0.01499164  0.01499414  0.24003118 -0.02396327 -0.02396327  0.01499164\\n\\\n",
      "          \\  0.01499414  0.0149917 ]\"\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 32032\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 32032\n",
      "    num_target_updates: 8\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.39\n",
      "    ram_util_percent: 18.0\n",
      "  pid: 1530\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.059792425053170443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03820303713800345\n",
      "    mean_inference_ms: 1.2719283107789934\n",
      "    mean_raw_obs_processing_ms: 0.15637991441071405\n",
      "  time_since_restore: 26.553105354309082\n",
      "  time_this_iter_s: 6.545329570770264\n",
      "  time_total_s: 26.553105354309082\n",
      "  timers:\n",
      "    learn_throughput: 3768.351\n",
      "    learn_time_ms: 8.492\n",
      "    load_throughput: 124080.362\n",
      "    load_time_ms: 0.258\n",
      "    update_time_ms: 1.084\n",
      "  timestamp: 1634743076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: d4611_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 4.4/30.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 0/0 GPUs, 0.0/15.76 GiB heap, 0.0/7.88 GiB objects<br>Result logdir: /home/ubuntu/ray_results/DQN<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00000</td><td>RUNNING   </td><td>10.0.33.251:1530</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         26.5531</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">-0.993424</td><td style=\"text-align: right;\">             1.41845</td><td style=\"text-align: right;\">           -23.7028 </td><td style=\"text-align: right;\">          20.5    </td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00001</td><td>TERMINATED</td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         26.3098</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\"> 0.173115</td><td style=\"text-align: right;\">             1.55262</td><td style=\"text-align: right;\">            -1.70087</td><td style=\"text-align: right;\">           9.58095</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00002</td><td>TERMINATED</td><td>                </td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         26.2375</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\"> 0.121214</td><td style=\"text-align: right;\">             1.54471</td><td style=\"text-align: right;\">            -2.79225</td><td style=\"text-align: right;\">          10.46   </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_SimpleCorridor_d4611_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-20_15-18-02\n",
      "  done: true\n",
      "  episode_len_mean: 8.147540983606557\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.573870675110745\n",
      "  episode_reward_mean: 0.31726598087463076\n",
      "  episode_reward_min: -1.5821095471002968\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 322\n",
      "  experiment_id: c51033ebd08941aaa937deaa8e824512\n",
      "  hostname: run-6170309a3ddbad0df8b110a0-tm44l\n",
      "  info:\n",
      "    last_target_update_ts: 5536\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.01\n",
      "          grad_gnorm: 0.007342854980379343\n",
      "          max_q: 0.8405561447143555\n",
      "          mean_q: 0.8152645230293274\n",
      "          min_q: 0.7226741313934326\n",
      "        mean_td_error: -0.044836193323135376\n",
      "        model: {}\n",
      "        td_error: \"[ 0.6119231   0.02110988  0.06569052  0.34596154  0.06569052 -0.7745429\\n\\\n",
      "          \\  0.06569052 -0.18270826 -0.77241945 -0.01714778  0.06569052  0.60437524\\n\\\n",
      "          \\  0.06569123  0.24945557  0.06569052  0.06569052  0.0274322  -0.9809735\\n\\\n",
      "          \\ -1.0926409   0.06569123 -0.00761163 -0.01714778  0.0274322  -0.007613\\n\\\n",
      "          \\ -0.2893262   0.06569052  0.0274322   0.06569123  0.02110988  0.06569123\\n\\\n",
      "          \\ -0.01714778  0.06569052]\"\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 40032\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 40032\n",
      "    num_target_updates: 10\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.0.33.251\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.942857142857143\n",
      "    ram_util_percent: 14.299999999999999\n",
      "  pid: 1530\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05873638954664783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.03766000439695193\n",
      "    mean_inference_ms: 1.245959205163397\n",
      "    mean_raw_obs_processing_ms: 0.15530608491368383\n",
      "  time_since_restore: 31.837305307388306\n",
      "  time_this_iter_s: 5.284199953079224\n",
      "  time_total_s: 31.837305307388306\n",
      "  timers:\n",
      "    learn_throughput: 3858.428\n",
      "    learn_time_ms: 8.294\n",
      "    load_throughput: 142557.332\n",
      "    load_time_ms: 0.224\n",
      "    update_time_ms: 1.129\n",
      "  timestamp: 1634743082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: d4611_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 4.4/30.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/6 CPUs, 0/0 GPUs, 0.0/15.76 GiB heap, 0.0/7.88 GiB objects<br>Result logdir: /home/ubuntu/ray_results/DQN<br>Number of trials: 3/3 (3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         31.8373</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">0.317266</td><td style=\"text-align: right;\">             1.57387</td><td style=\"text-align: right;\">            -1.58211</td><td style=\"text-align: right;\">           8.14754</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         26.3098</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">0.173115</td><td style=\"text-align: right;\">             1.55262</td><td style=\"text-align: right;\">            -1.70087</td><td style=\"text-align: right;\">           9.58095</td></tr>\n",
       "<tr><td>DQN_SimpleCorridor_d4611_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         26.2375</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">0.121214</td><td style=\"text-align: right;\">             1.54471</td><td style=\"text-align: right;\">            -2.79225</td><td style=\"text-align: right;\">          10.46   </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 15:18:02,573\tINFO tune.py:617 -- Total run time: 37.74 seconds (37.40 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "best config:  {'env': <class '__main__.SimpleCorridor'>, 'env_config': {'corridor_length': 5}, 'num_gpus': 0, 'model': {'custom_model': 'my_model', 'vf_share_layers': True}, 'lr': 0.01, 'num_workers': 1, 'framework': 'torch'}\n"
     ]
    }
   ],
   "source": [
    "## run and create the environment; change to work in a notebook\n",
    "\n",
    "import ray\n",
    "from ray.tune import grid_search\n",
    "\n",
    "#ray.init()\n",
    "\n",
    "# Can also register the env creator function explicitly with:\n",
    "# register_env(\"corridor\", lambda config: SimpleCorridor(config))\n",
    "\n",
    "ModelCatalog.register_custom_model(\n",
    "    \"my_model\", TorchCustomModel\n",
    "    if framework == \"torch\" else CustomModel)\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleCorridor,  # or \"corridor\" if registered above\n",
    "    \"env_config\": {\n",
    "        \"corridor_length\": 5,\n",
    "    },\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"my_model\",\n",
    "        \"vf_share_layers\": True,\n",
    "    },\n",
    "    \"lr\": grid_search([1e-2, 1e-4, 1e-6]),  # try different lrs\n",
    "    \"num_workers\": 1,  # parallelism\n",
    "    \"framework\": framework,\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": stop_iters,\n",
    "    \"timesteps_total\": stop_timesteps,\n",
    "    \"episode_reward_mean\": stop_reward,\n",
    "}\n",
    "\n",
    "results = tune.run(run, config=config, stop=stop)\n",
    "\n",
    "if as_test:\n",
    "    check_learning_achieved(results, stop_reward)\n",
    "    \n",
    "print(\"best config: \", results.get_best_config(metric = 'episode_reward_mean', mode = 'max'))\n",
    "\n",
    "#ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finance Example: Predicting Investing Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "\n",
    "We have two notebooks remaining:\n",
    "\n",
    "*Using Pytorch and Ray for a simple finance example using DQN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
